{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f72e90",
   "metadata": {},
   "source": [
    "# AI 챗봇 개발 기초"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c09914",
   "metadata": {},
   "source": [
    "## 챗봇 기본 개념 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd401dcd",
   "metadata": {},
   "source": [
    "### 의도 분류 기본 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e9298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반갑습니다!\n",
      "날씨 정보를 조회하겠습니다.\n"
     ]
    }
   ],
   "source": [
    "class BasicChatbot:\n",
    "    def __init__(self):\n",
    "        self.intents = {\n",
    "            \"greeting\": [\"안녕\", \"hi\", \"hello\", \"반가워\"],\n",
    "            \"weather\": [\"날씨\", \"weather\", \"비\", \"맑아\"],\n",
    "            \"goodbye\": [\"안녕히\", \"bye\", \"goodbye\", \"잘가\"]\n",
    "        }\n",
    "\n",
    "        self.responses = {\n",
    "            \"greeting\": [\"안녕하세요! 무엇을 도와드릴까요?\", \"반갑습니다!\"],\n",
    "            \"weather\": [\"날씨 정보를 조회하겠습니다.\", \"어느 지역의 날씨를 알고 싶으신가요?\"],\n",
    "            \"goodbye\": [\"안녕히 가세요!\", \"좋은 하루 되세요!\"],\n",
    "            \"default\": [\"죄송합니다. 이해하지 못했습니다.\", \"다시 말씀해 주시겠어요?\"]\n",
    "        }\n",
    "    \n",
    "    # 의도 분류 함수\n",
    "    def classify_intent(self, user_input):\n",
    "        for intent, keywords in self.intents.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in user_input.lower():\n",
    "                    \n",
    "                    return intent\n",
    "        return \"default\"\n",
    "\n",
    "    # 응답 생성 함수\n",
    "    def generate_response(self, intent):\n",
    "        import random\n",
    "        \n",
    "        responses = self.responses.get(intent, self.responses[\"default\"])\n",
    "        return random.choice(responses)\n",
    "    \n",
    "    def chat(self, user_input):\n",
    "        intent = self.classify_intent(user_input)\n",
    "        response = self.generate_response(intent)\n",
    "        return response\n",
    "    \n",
    "bot = BasicChatbot()\n",
    "print(bot.chat(\"안녕\"))  \n",
    "print(bot.chat(\"오늘 날씨 어때?\"))\n",
    "print(bot.chat(\"잘가\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27df22",
   "metadata": {},
   "source": [
    "### BERT 기반 의도 분류 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eca6acc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/hs/81ncphtd2yzggndhp3qhjq400000gn/T/ipykernel_3871/777990698.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodings: {'input_ids': tensor([[    2,  5891,  2205,  5971,     3,     0],\n",
      "        [    2,  3822,  5792, 10604,    35,     3],\n",
      "        [    2,  1521,   543,     3,     0,     0],\n",
      "        [    2, 28542,     3,     0,     0,     0],\n",
      "        [    2,  1187, 31444,    35,     3,     0],\n",
      "        [    2, 26382, 20260,     3,     0,     0],\n",
      "        [    2,  4867, 19521,  2585, 10283,     3],\n",
      "        [    2,  7301,  4212,  2810,     3,     0],\n",
      "        [    2,  4182,  4635,  2097,  2810,     3],\n",
      "        [    2,  3771,  4635,  2097,  2810,     3],\n",
      "        [    2,  5396,  3897,  2810,     3,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]])}\n",
      "encoded_labels: [1 5 0 1 5 0 2 2 3 4 2]\n",
      "Epoch 1/3\n",
      "Average loss: 1.8712\n",
      "훈련 완료\n",
      "Epoch 2/3\n",
      "Average loss: 1.6928\n",
      "훈련 완료\n",
      "Epoch 3/3\n",
      "Average loss: 1.5608\n",
      "훈련 완료\n",
      "inputs: {'input_ids': tensor([[    2,  3822,  5792, 10604,    35,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "Predicted intent: weather, Confidence: 0.39\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class IntentClassifier:\n",
    "    \"\"\"BERT 기반 의도 분류기\"\"\"\n",
    "    def __init__(self, model_name = \"klue/bert-base\"):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def prepare_data(self, texts, labels):\n",
    "        \"\"\"데이터 전처리 및 라벨 인코딩\"\"\"\n",
    "        encoded_labels = self.label_encoder.fit_transform(labels)\n",
    "        # 토큰화\n",
    "        encodings = self.tokenizer(\n",
    "            texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\"\n",
    "        )\n",
    "        print(f\"encodings: {encodings}\")\n",
    "        print(f\"encoded_labels: {encoded_labels}\")\n",
    "        return encodings, encoded_labels\n",
    "    \n",
    "    def train(self, train_texts, train_labels):\n",
    "        \"\"\"모델 학습\"\"\"\n",
    "        \n",
    "        num_labels = len(set(train_labels))\n",
    "        \n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            \"klue/bert-base\", num_labels=num_labels\n",
    "        )\n",
    "        \n",
    "        # 데이터 준비\n",
    "        train_encodings, train_labels = self.prepare_data(train_texts, train_labels)\n",
    "\n",
    "        class IntentDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, encodings, labels):\n",
    "                self.encodings = encodings\n",
    "                self.labels = labels\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "                item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "                return item\n",
    "                \n",
    "            def __len__(self):\n",
    "                return len(self.labels)\n",
    "                \n",
    "        train_dataset = IntentDataset(train_encodings, train_labels)\n",
    "            \n",
    "        batch_size = 16\n",
    "        epochs = 3\n",
    "        learning_rate = 2e-5\n",
    "        weight_decay = 0.01\n",
    "            \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                train_dataset, batch_size=batch_size, shuffle=True\n",
    "                )\n",
    "        optimizer = torch.optim.AdamW(\n",
    "                self.model.parameters(), \n",
    "                lr=learning_rate, \n",
    "                weight_decay=weight_decay\n",
    "                )\n",
    "            \n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                    \n",
    "                loss.backward()\n",
    "                    \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "            print(\"훈련 완료\")\n",
    "            \n",
    "    def predict(self, text):\n",
    "        \"\"\"의도 예측\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        print(f\"inputs: {inputs}\")\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        confidence = torch.max(predictions).item()\n",
    "\n",
    "        intent = self.label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "        return intent, confidence\n",
    "        \n",
    "# 예제 데이터\n",
    "train_texts = [\n",
    "    \"안녕하세요\", \"오늘 날씨 어때?\", \"잘 가\", \"반가워\", \"비 올까?\", \"안녕히 계세요\",\n",
    "    \"주문하고싶어요\", \"배달 시켜줘\", \"음식 추천해줘\", \"영화 추천해줘\",\"메뉴 보여줘\"\n",
    "]\n",
    "\n",
    "train_labels = [\n",
    "    \"greeting\", \"weather\", \"goodbye\", \"greeting\", \"weather\", \"goodbye\",\n",
    "    \"order_food\", \"order_food\", \"recommend_food\", \"recommend_movie\", \"order_food\"\n",
    "]\n",
    "\n",
    "classifier = IntentClassifier()\n",
    "\n",
    "classifier.train(train_texts, train_labels)\n",
    "\n",
    "intent, confidence = classifier.predict(\"오늘 날씨 어때?\")\n",
    "print(f\"Predicted intent: {intent}, Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f225d",
   "metadata": {},
   "source": [
    "### spacy/ 개체 검출 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd1d8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6b3d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '내일 서울의', 'label': 'PER', 'start': 0, 'end': 6, 'description': 'Named person or family.'}]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import spacy.cli  # ← 함수 밖(전역)에서 임포트 권장\n",
    "\n",
    "class EntityRecognizer:\n",
    "    def __init__(self, lang='xx_ent_wiki_sm'):  # 다국어 NER\n",
    "        self.nlp = self._load_pipeline(lang, fallback='en_core_web_sm')\n",
    "\n",
    "    def _load_pipeline(self, name, fallback=None):\n",
    "        # 1차 시도\n",
    "        try:\n",
    "            return spacy.load(name)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        # 없으면 다운로드 후 재시도\n",
    "        try:\n",
    "            spacy.cli.download(name)\n",
    "            return spacy.load(name)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 폴백 모델 시도\n",
    "        if fallback:\n",
    "            try:\n",
    "                spacy.cli.download(fallback)\n",
    "                return spacy.load(fallback)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # 마지막 폴백: 토크나이저만\n",
    "        return spacy.blank(\"xx\")\n",
    "\n",
    "    def extract_entities(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        return [\n",
    "            {\n",
    "                \"text\": ent.text,\n",
    "                \"label\": ent.label_,\n",
    "                \"start\": ent.start_char,\n",
    "                \"end\": ent.end_char,\n",
    "                \"description\": spacy.explain(ent.label_) or \"\",\n",
    "            }\n",
    "            for ent in doc.ents\n",
    "        ]\n",
    "\n",
    "extractor = EntityRecognizer()\n",
    "text = \"내일 서울의 날씨는 어때.\"\n",
    "print(extractor.extract_entities(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883c843",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
